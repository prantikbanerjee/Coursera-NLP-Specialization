1) In Week1, I learned about neural networks for deep learning and built a sophisticated tweet classifier that places 
tweets into positive or negative sentiment categories, using a deep neural network. 

2) In Week2, I learned about the limitations of traditional language models like how RNNs and GRUs use sequential data 
for text prediction. Then built my own next-word generator using a simple RNN on Shakespeare text data

3) In Week3, I learned about LSTMs, which solves vanishnig gradient problem and how Named Entity Recognision systems
quickly extract important imformation from text. Then built my own NER using an LSTM.

4) In Week4, I learned about Siamese networks, a special type of neural network made of two identical networks 
that are eventually merged together, known as Siamese network that identifies question duplicates in a dataset from Quora.
