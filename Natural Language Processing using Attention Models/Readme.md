1) In Week1, I learned traditional seq2seq model and how to solve for them by adding an attention mechanism, 
then built a Neural Machine Translation model with Attention that translates English sentences into German.

2) In Week2, I learned about MultiHead attention meachanism, and built a tool that generates text summaries.

3) In Week3, I explored about transfer learning with state-of-the-art models like T5 and BERT, then built 
a model that can answer questions.

4) Finally, In Week4, I learned about some limitations of transformer model and their solutions, then built
a chatbot using a Reformer model.
